{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-19T17:50:21.442972Z","iopub.execute_input":"2022-03-19T17:50:21.443535Z","iopub.status.idle":"2022-03-19T17:50:21.470215Z","shell.execute_reply.started":"2022-03-19T17:50:21.443448Z","shell.execute_reply":"2022-03-19T17:50:21.469607Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"! python -m spacy download en_core_web_lg","metadata":{"execution":{"iopub.status.busy":"2022-03-19T17:50:21.471584Z","iopub.execute_input":"2022-03-19T17:50:21.472116Z","iopub.status.idle":"2022-03-19T17:51:53.864331Z","shell.execute_reply.started":"2022-03-19T17:50:21.472086Z","shell.execute_reply":"2022-03-19T17:51:53.863235Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"**Load Data**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt \nimport seaborn as sns \n\nimport re \nimport collections\n\nimport nltk \nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords \n\nimport spacy \nimport emoji \n\nimport unicodedata\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression , Lasso , Huber , RidgeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n\nimport tensorflow as tf ","metadata":{"execution":{"iopub.status.busy":"2022-03-19T17:51:53.866548Z","iopub.execute_input":"2022-03-19T17:51:53.866843Z","iopub.status.idle":"2022-03-19T17:51:59.415854Z","shell.execute_reply.started":"2022-03-19T17:51:53.866810Z","shell.execute_reply":"2022-03-19T17:51:59.415009Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"nlp = spacy.load('en_core_web_lg')","metadata":{"execution":{"iopub.status.busy":"2022-03-19T17:51:59.417254Z","iopub.execute_input":"2022-03-19T17:51:59.417576Z","iopub.status.idle":"2022-03-19T17:52:05.306299Z","shell.execute_reply.started":"2022-03-19T17:51:59.417536Z","shell.execute_reply":"2022-03-19T17:52:05.305408Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest_data = pd.read_csv('../input/nlp-getting-started/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-03-19T17:52:05.308207Z","iopub.execute_input":"2022-03-19T17:52:05.308462Z","iopub.status.idle":"2022-03-19T17:52:05.373750Z","shell.execute_reply.started":"2022-03-19T17:52:05.308431Z","shell.execute_reply":"2022-03-19T17:52:05.372740Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-19T17:52:05.375183Z","iopub.execute_input":"2022-03-19T17:52:05.375456Z","iopub.status.idle":"2022-03-19T17:52:05.395748Z","shell.execute_reply.started":"2022-03-19T17:52:05.375423Z","shell.execute_reply":"2022-03-19T17:52:05.395116Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Methods For Cleaning Tweets\n\nFor this notebook, I will follow the below sequence of operations for cleaning and removing duplicate tweets.\n\n1. For this notebook, I will follow the below sequence of operations for cleaning and removing duplicate tweets.\n2. Clean URL's/Hyperlinks from tweets and look for duplicates.\n3. Separate tweets based on category and do some exploratory analysis.\n4. Write methods for specific cases e.g removing URLs, removing emojis etc.\n5. Clean both the train and test tweets and again look for duplicates in training data.","metadata":{}},{"cell_type":"code","source":"def remove_urls(text):\n    \"\"\"Remove any URL/Hyperlink in the tweet\"\"\"\n    text = re.sub(r\"(https?:\\/\\/)(\\s)*(www\\.)?(\\s)*((\\w|\\s)+\\.)*([\\w\\-\\s]+\\/)*([\\w\\-]+)((\\?)?[\\w\\s]*=\\s*[\\w\\%&]*)*\",\"\",text)\n    return text\n\ndef clean_punc_url_stopw_single(text):\n    \"\"\" Using SpaCy pipeline to clean and lemmatize\n    \n    Using SpaCy pipeline to iterate over tweet words\n    and remove stopwords/URLs/email/punctuation\n    and also lemmatize tweet.\n    \"\"\"\n    \n    clean_text = \"\"\n    doc_text = nlp(text)\n    for token in doc_text:\n        if(token.like_url or token.like_email or token.is_punct or token.is_stop):\n            continue\n        else:\n            clean_text += token.lemma_ + \" \"\n    return clean_text[:-1].strip()\n\ndef clean_tags_mentions_single(txt):\n    \"\"\"Using Regex to remove hashtags/mentions\n    \n    Remove any hashtag/mention from tweet\n    plus some more cleaning.\n    \"\"\"\n    patt_mention = r\"[@]\\w+\"\n    patt_tags = r\"[#]\\w+\"\n    clean_str = re.sub(patt_mention, \"\", txt)\n    clean_str = re.sub(patt_tags, \"\", clean_str)\n    clean_str = re.sub(r'[0-9]+', '', clean_str)\n    clean_str = re.sub(\"'ve\", \" have \", clean_str)\n    clean_str = re.sub(\"&amp;\", \"\", clean_str)\n    clean_str = re.sub(\"\\n\", \"\", clean_str)\n    return \" \".join(clean_str.split())\n\n\n\ndef remove_emojis_single(text):\n    \"\"\"Remove emojis from tweet\"\"\"\n    return emoji.get_emoji_regexp().sub(u'' , text)\n\n\ndef remove_accents(string):\n    \"\"\"Remove accents(á, é, í etc) from tweet\"\"\"\n    nfkd_form  = unicodedata.normalize('NFKD' , string)\n    return u\"\".join([c for c in nfkd_form if not unicodedata.combining(c)])\n\ndef remove_single_chars(text):\n    \"\"\"Remove any stray single characters\"\"\"\n    return ' '.join( [w for w in text.split() if len(w)>1] )\n\ndef remove_non_ascii(text):\n    \"\"\"Remove any non-ascii characters\"\"\"\n    return re.sub(r'[^\\x00-\\x7F]+','', text)\n\ndef remove_remaining_punct(text):\n    \"\"\"Remove any remaining puctuation\"\"\"\n    res1 = re.sub(r\"[!@#$%^&*()_\\-=+}{\\[\\]|\\\\/<>,.?~`';]+\", \" \", text)\n    res2 = \" \".join(res1.split())\n    return res2\n\ndef clean_tweets_efficient(all_tweets):\n    ''' Combine individual methods\n      \n    Use all the cleaning functions to remove emojis, punctuations\n    hyperlinks, stopwords, tags, mentions and non-english words/characters\n    '''\n    clean_tweets = list()\n    for tweet in all_tweets:\n        non_accents = remove_accents(remove_non_ascii(tweet))\n        clean_level_1 = remove_emojis_single(non_accents)\n        clean_level_2 = clean_tags_mentions_single(clean_level_1)\n        clean_level_3 = clean_punc_url_stopw_single(clean_level_2)\n        clean_level_4 = remove_single_chars(clean_level_3)\n        clean_tweets.append(clean_level_4)\n        \n    return clean_tweets\n\n\ndef get_wrd_count(text_lst):\n    \"\"\"Get Word counters for EDA\"\"\"\n    all_words = []\n    tokenizer = RegexpTokenizer(r'\\w+')\n    for txt in text_lst:\n        words = tokenizer.tokenize(txt)\n        all_words.extend(words)\n    word_counter = collections.Counter(all_words)\n    return word_counter","metadata":{"execution":{"iopub.status.busy":"2022-03-19T17:52:05.397118Z","iopub.execute_input":"2022-03-19T17:52:05.397457Z","iopub.status.idle":"2022-03-19T17:52:05.421304Z","shell.execute_reply.started":"2022-03-19T17:52:05.397423Z","shell.execute_reply":"2022-03-19T17:52:05.420450Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"clean_urls_train = train_data['text'].apply(lambda x : remove_urls(x))","metadata":{"execution":{"iopub.status.busy":"2022-03-19T17:52:05.422927Z","iopub.execute_input":"2022-03-19T17:52:05.423190Z","iopub.status.idle":"2022-03-19T17:52:05.473220Z","shell.execute_reply.started":"2022-03-19T17:52:05.423159Z","shell.execute_reply":"2022-03-19T17:52:05.472361Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_data_duplicates = train_data[clean_urls_train.duplicated()]","metadata":{"execution":{"iopub.status.busy":"2022-03-19T17:52:05.474721Z","iopub.execute_input":"2022-03-19T17:52:05.474969Z","iopub.status.idle":"2022-03-19T17:52:05.484397Z","shell.execute_reply.started":"2022-03-19T17:52:05.474941Z","shell.execute_reply":"2022-03-19T17:52:05.483697Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# let's have a quick look at duplicates after removing urls\nprint(\"total duplicates found after removing urls : {}\".format(train_data_duplicates.shape[0]))","metadata":{"execution":{"iopub.status.busy":"2022-03-19T17:52:05.485617Z","iopub.execute_input":"2022-03-19T17:52:05.485935Z","iopub.status.idle":"2022-03-19T17:52:05.497832Z","shell.execute_reply.started":"2022-03-19T17:52:05.485905Z","shell.execute_reply":"2022-03-19T17:52:05.496932Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_data['text'] = train_data['text'].apply(lambda x : remove_urls(x))\ntest_data['clean_text'] = test_data['text'].apply(lambda x : remove_urls(x))","metadata":{"execution":{"iopub.status.busy":"2022-03-19T17:52:05.500688Z","iopub.execute_input":"2022-03-19T17:52:05.500913Z","iopub.status.idle":"2022-03-19T17:52:05.559819Z","shell.execute_reply.started":"2022-03-19T17:52:05.500885Z","shell.execute_reply":"2022-03-19T17:52:05.559135Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_data = train_data.drop_duplicates(subset='text')","metadata":{"execution":{"iopub.status.busy":"2022-03-19T17:52:05.560812Z","iopub.execute_input":"2022-03-19T17:52:05.561374Z","iopub.status.idle":"2022-03-19T17:52:05.570865Z","shell.execute_reply.started":"2022-03-19T17:52:05.561340Z","shell.execute_reply":"2022-03-19T17:52:05.570237Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#separate tweets by category\ntweets_0 = train_data[train_data['target']==0]\ntweets_1 = train_data[train_data['target']==1]\ntweets_test = test_data['clean_text']","metadata":{"execution":{"iopub.status.busy":"2022-03-19T17:52:05.571788Z","iopub.execute_input":"2022-03-19T17:52:05.572636Z","iopub.status.idle":"2022-03-19T17:52:05.582472Z","shell.execute_reply.started":"2022-03-19T17:52:05.572602Z","shell.execute_reply":"2022-03-19T17:52:05.581748Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"tweets_0_text_lst = tweets_0['text'].tolist()\ntweets_1_text_lst = tweets_1['text'].tolist()\ntweets_test_text_lst = tweets_test.tolist()","metadata":{"execution":{"iopub.status.busy":"2022-03-19T17:52:05.583875Z","iopub.execute_input":"2022-03-19T17:52:05.584380Z","iopub.status.idle":"2022-03-19T17:52:05.593590Z","shell.execute_reply.started":"2022-03-19T17:52:05.584335Z","shell.execute_reply":"2022-03-19T17:52:05.592845Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"#clean tweets for both training and testing data \nclean_tweets_0 = clean_tweets_efficient(tweets_0_text_lst)\nclean_tweets_1 = clean_tweets_efficient(tweets_1_text_lst)\nclean_test_tweets = clean_tweets_efficient(tweets_test_text_lst)","metadata":{"execution":{"iopub.status.busy":"2022-03-19T17:52:05.595054Z","iopub.execute_input":"2022-03-19T17:52:05.595602Z","iopub.status.idle":"2022-03-19T17:53:55.460713Z","shell.execute_reply.started":"2022-03-19T17:52:05.595556Z","shell.execute_reply":"2022-03-19T17:53:55.459807Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#combine and look for duplicates \nall_cleaned_tweets = clean_tweets_0 + clean_tweets_1\nall_cleaned_tweets_with_test = clean_tweets_0 + clean_tweets_1 + clean_test_tweets #useful while modeling \nall_labels = [0]*len(clean_tweets_0) + [1]*len(clean_tweets_1)","metadata":{"execution":{"iopub.status.busy":"2022-03-19T17:53:55.461837Z","iopub.execute_input":"2022-03-19T17:53:55.462072Z","iopub.status.idle":"2022-03-19T17:53:55.467653Z","shell.execute_reply.started":"2022-03-19T17:53:55.462043Z","shell.execute_reply":"2022-03-19T17:53:55.466385Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"df_clean_tweets = pd.DataFrame(data={\"text\" :all_cleaned_tweets , \"label\":all_labels})","metadata":{"execution":{"iopub.status.busy":"2022-03-19T17:53:55.468950Z","iopub.execute_input":"2022-03-19T17:53:55.469187Z","iopub.status.idle":"2022-03-19T17:53:55.486076Z","shell.execute_reply.started":"2022-03-19T17:53:55.469156Z","shell.execute_reply":"2022-03-19T17:53:55.485372Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"df_clean_tweets.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-19T17:53:55.487466Z","iopub.execute_input":"2022-03-19T17:53:55.487865Z","iopub.status.idle":"2022-03-19T17:53:55.497772Z","shell.execute_reply.started":"2022-03-19T17:53:55.487831Z","shell.execute_reply":"2022-03-19T17:53:55.496929Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"df_clean_tweets[df_clean_tweets.duplicated(subset='text')]\n# Again, we should have a look at our duplicates","metadata":{"execution":{"iopub.status.busy":"2022-03-19T17:53:55.498977Z","iopub.execute_input":"2022-03-19T17:53:55.499394Z","iopub.status.idle":"2022-03-19T17:53:55.521413Z","shell.execute_reply.started":"2022-03-19T17:53:55.499358Z","shell.execute_reply":"2022-03-19T17:53:55.520479Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"df_clean_tweets_dedep = df_clean_tweets.drop_duplicates(subset='text')","metadata":{"execution":{"iopub.status.busy":"2022-03-19T17:53:55.523050Z","iopub.execute_input":"2022-03-19T17:53:55.523577Z","iopub.status.idle":"2022-03-19T17:53:55.534937Z","shell.execute_reply.started":"2022-03-19T17:53:55.523534Z","shell.execute_reply":"2022-03-19T17:53:55.533782Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Remaining training data\nprint(df_clean_tweets_dedep.shape)\nprint(df_clean_tweets_dedep[df_clean_tweets_dedep['label']==0].shape)\nprint(df_clean_tweets_dedep[df_clean_tweets_dedep['label']==1].shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-19T17:53:55.537151Z","iopub.execute_input":"2022-03-19T17:53:55.537518Z","iopub.status.idle":"2022-03-19T17:53:55.549511Z","shell.execute_reply.started":"2022-03-19T17:53:55.537474Z","shell.execute_reply":"2022-03-19T17:53:55.548355Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"**EDA**","metadata":{}},{"cell_type":"code","source":"sns.countplot(x='label' , data=df_clean_tweets_dedep);\nplt.title(\"Target Distribution\");","metadata":{"execution":{"iopub.status.busy":"2022-03-19T17:53:55.552071Z","iopub.execute_input":"2022-03-19T17:53:55.552634Z","iopub.status.idle":"2022-03-19T17:53:55.789240Z","shell.execute_reply.started":"2022-03-19T17:53:55.552594Z","shell.execute_reply":"2022-03-19T17:53:55.788367Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"tweets_0_cleaned = df_clean_tweets_dedep[df_clean_tweets_dedep['label']==0]\ntweets_1_cleaned = df_clean_tweets_dedep[df_clean_tweets_dedep['label']==1]","metadata":{"execution":{"iopub.status.busy":"2022-03-19T17:53:55.791015Z","iopub.execute_input":"2022-03-19T17:53:55.791530Z","iopub.status.idle":"2022-03-19T17:53:55.800609Z","shell.execute_reply.started":"2022-03-19T17:53:55.791483Z","shell.execute_reply":"2022-03-19T17:53:55.799475Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"tweet_0_len = tweets_0_cleaned['text'].apply(lambda x:len(x) )\ntweet_1_len = tweets_1_cleaned['text'].apply(lambda x:len(x) )\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\nsns.histplot(tweet_0_len, ax=ax[0]);\nax[0].set_title(\"Tweet Length distribution for Non Disaster Tweets\");\nsns.histplot(tweet_1_len, ax=ax[1]);\nax[1].set_title(\"Tweet Length distribution for Disaster Tweets\");\nplt.tight_layout();","metadata":{"execution":{"iopub.status.busy":"2022-03-19T17:53:55.802654Z","iopub.execute_input":"2022-03-19T17:53:55.803256Z","iopub.status.idle":"2022-03-19T17:53:56.414468Z","shell.execute_reply.started":"2022-03-19T17:53:55.803205Z","shell.execute_reply":"2022-03-19T17:53:56.413604Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"#Common words for each category\n\ntweet0_wrd = get_wrd_count(tweets_0_cleaned['text'].tolist())\ntweet1_wrd = get_wrd_count(tweets_1_cleaned['text'].tolist())\n\ntweet0_wrd_cnt_sorted = tweet0_wrd.most_common(n=10)\ntweet1_wrd_cnt_sorted = tweet1_wrd.most_common(n=10)\nl0, h0, l1, h1 = [],[],[],[]\n_ = [(l0.append(i[0]), h0.append(i[1])) for i in tweet0_wrd_cnt_sorted]\n_ = [(l1.append(i[0]), h1.append(i[1])) for i in tweet1_wrd_cnt_sorted]\n#print(l1, h1)\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(13, 6));\nsns.barplot(x=list(range(len(l0))), y=h0, ax=ax[0]);\nax[0].set_ylim(top=300);\nax[0].set_xticks(ticks = list(range(len(l0))));\nax[0].set_xticklabels(l0);\nax[0].set_xlabel('Words');\nax[0].set_ylabel('Count');\nax[0].set_title(\"Most common words for non disaster tweets\");\n\nsns.barplot(x=list(range(len(l1))), y=h1, ax=ax[1]);\nax[1].set_ylim(top=300);\nax[1].set_xticks(ticks = list(range(len(l1))));\nax[1].set_xticklabels(l1);\nax[1].set_xlabel('Words');\nax[1].set_ylabel('Count');\nax[1].set_title(\"Most common words for disaster tweets\");","metadata":{"execution":{"iopub.status.busy":"2022-03-19T17:53:56.415806Z","iopub.execute_input":"2022-03-19T17:53:56.416304Z","iopub.status.idle":"2022-03-19T17:53:56.859425Z","shell.execute_reply.started":"2022-03-19T17:53:56.416267Z","shell.execute_reply":"2022-03-19T17:53:56.858564Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"> **Modeling**","metadata":{}},{"cell_type":"markdown","source":"**Baseline with TF-IDF and Logistic Classifier**","metadata":{}},{"cell_type":"code","source":"all_cleaned_text = df_clean_tweets_dedep['text'].tolist()\nall_cleaned_text_with_test = df_clean_tweets_dedep['text'].tolist() + clean_test_tweets\nall_targets = df_clean_tweets_dedep['label'].tolist()","metadata":{"execution":{"iopub.status.busy":"2022-03-19T17:53:56.860754Z","iopub.execute_input":"2022-03-19T17:53:56.861058Z","iopub.status.idle":"2022-03-19T17:53:56.866805Z","shell.execute_reply.started":"2022-03-19T17:53:56.861015Z","shell.execute_reply":"2022-03-19T17:53:56.865986Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"tfidf = TfidfVectorizer(ngram_range=(2,4) , max_df=1000 , min_df=10)\ntfidf.fit(all_cleaned_text)","metadata":{"execution":{"iopub.status.busy":"2022-03-19T17:53:56.867887Z","iopub.execute_input":"2022-03-19T17:53:56.868671Z","iopub.status.idle":"2022-03-19T17:53:57.225143Z","shell.execute_reply.started":"2022-03-19T17:53:56.868636Z","shell.execute_reply":"2022-03-19T17:53:57.224301Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"tfidf_feats = tfidf.transform(all_cleaned_text)","metadata":{"execution":{"iopub.status.busy":"2022-03-19T17:53:57.226586Z","iopub.execute_input":"2022-03-19T17:53:57.226810Z","iopub.status.idle":"2022-03-19T17:53:57.411060Z","shell.execute_reply.started":"2022-03-19T17:53:57.226782Z","shell.execute_reply":"2022-03-19T17:53:57.410411Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"trainx , testvalx , trainy , testvaly = train_test_split(tfidf_feats , all_targets , test_size=0.4)\nvalx , testx , valy , testy = train_test_split(testvalx , testvaly , test_size=0.5)","metadata":{"execution":{"iopub.status.busy":"2022-03-19T17:53:57.414550Z","iopub.execute_input":"2022-03-19T17:53:57.415432Z","iopub.status.idle":"2022-03-19T17:53:57.425658Z","shell.execute_reply.started":"2022-03-19T17:53:57.415391Z","shell.execute_reply":"2022-03-19T17:53:57.424665Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"lr = LogisticRegression(C=10)\nlr.fit(trainx , trainy)","metadata":{"execution":{"iopub.status.busy":"2022-03-19T17:53:57.427143Z","iopub.execute_input":"2022-03-19T17:53:57.427463Z","iopub.status.idle":"2022-03-19T17:53:57.491166Z","shell.execute_reply.started":"2022-03-19T17:53:57.427432Z","shell.execute_reply":"2022-03-19T17:53:57.490097Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"print(f\"Validation Set Score: {lr.score(valx, valy)}\")\nprint(f\"Test Set Score: {lr.score(testx, testy)}\")","metadata":{"execution":{"iopub.status.busy":"2022-03-19T17:53:57.492539Z","iopub.execute_input":"2022-03-19T17:53:57.492799Z","iopub.status.idle":"2022-03-19T17:53:57.502942Z","shell.execute_reply.started":"2022-03-19T17:53:57.492767Z","shell.execute_reply":"2022-03-19T17:53:57.501985Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"print(classification_report(testy, lr.predict(testx)))\n","metadata":{"execution":{"iopub.status.busy":"2022-03-19T17:53:57.504107Z","iopub.execute_input":"2022-03-19T17:53:57.504447Z","iopub.status.idle":"2022-03-19T17:53:57.520789Z","shell.execute_reply.started":"2022-03-19T17:53:57.504411Z","shell.execute_reply":"2022-03-19T17:53:57.520139Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"**RNN With GloVe Embeddings**","metadata":{}},{"cell_type":"code","source":"# Download and extract GloVe embeddings\n\n!wget http://nlp.stanford.edu/data/glove.6B.zip\n!unzip -q glove.6B.zip","metadata":{"execution":{"iopub.status.busy":"2022-03-19T17:53:57.522298Z","iopub.execute_input":"2022-03-19T17:53:57.522980Z","iopub.status.idle":"2022-03-19T17:57:09.199861Z","shell.execute_reply.started":"2022-03-19T17:53:57.522941Z","shell.execute_reply":"2022-03-19T17:57:09.197913Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# load pretrained Glove embeddings \ndict_w2v = {}\nwith open('./glove.6B.100d.txt' , \"r\") as file :\n    for line in file :\n        tokens = line.split()\n        word = tokens[0]\n        vector = np.array(tokens[1:] , dtype = np.float32)\n        if vector.shape[0] == 100:\n            dict_w2v[word] = vector\n        else:\n            print(\"there was an issue with \" + word)\n#let's check the vocab size \nprint(\"Dictionary Size : \" , len(dict_w2v))","metadata":{"execution":{"iopub.status.busy":"2022-03-19T17:57:09.203422Z","iopub.execute_input":"2022-03-19T17:57:09.203931Z","iopub.status.idle":"2022-03-19T17:57:21.923829Z","shell.execute_reply.started":"2022-03-19T17:57:09.203871Z","shell.execute_reply":"2022-03-19T17:57:21.922888Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=\"\")\n\n# We use entire data  training + test to fit tokenizer\n# so we can use it for predicting as well\n\ntokenizer.fit_on_texts(all_cleaned_text_with_test)","metadata":{"execution":{"iopub.status.busy":"2022-03-19T17:57:21.925337Z","iopub.execute_input":"2022-03-19T17:57:21.925621Z","iopub.status.idle":"2022-03-19T17:57:23.220504Z","shell.execute_reply.started":"2022-03-19T17:57:21.925582Z","shell.execute_reply":"2022-03-19T17:57:23.219382Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"NUM_WORDS = len(tokenizer.word_index) + 1\nNUM_CLS = 2\nMAX_LEN = 25\nprint(\"Total Words in tokenizer : {}\".format(NUM_WORDS))","metadata":{"execution":{"iopub.status.busy":"2022-03-19T17:57:23.221933Z","iopub.execute_input":"2022-03-19T17:57:23.222207Z","iopub.status.idle":"2022-03-19T17:57:23.229888Z","shell.execute_reply.started":"2022-03-19T17:57:23.222174Z","shell.execute_reply":"2022-03-19T17:57:23.228937Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"Load Embeddings","metadata":{}},{"cell_type":"code","source":"embedding_dim = 100\nembedding_matrix = np.zeros((NUM_WORDS , embedding_dim))\n\nunk_cnt = 0\nunk_set = set()\nfor word in tokenizer.word_index.keys():\n    embedding_vector = dict_w2v.get(word)\n    if embedding_vector is not None:\n        tkn_id = tokenizer.word_index[word]\n        embedding_matrix[tkn_id] = embedding_vector\n    else:\n        unk_cnt += 1\n        unk_set.add(word)\n    \n# Print how many weren't found\nprint(\"Total unknown words: \", unk_cnt)","metadata":{"execution":{"iopub.status.busy":"2022-03-19T17:57:23.231431Z","iopub.execute_input":"2022-03-19T17:57:23.232732Z","iopub.status.idle":"2022-03-19T17:57:23.297504Z","shell.execute_reply.started":"2022-03-19T17:57:23.232675Z","shell.execute_reply":"2022-03-19T17:57:23.296562Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"Convert strings to indices and pad so all sequences are of same length\n\n","metadata":{}},{"cell_type":"code","source":"all_sequences = tokenizer.texts_to_sequences(all_cleaned_text)\nall_padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(all_sequences, maxlen=MAX_LEN)","metadata":{"execution":{"iopub.status.busy":"2022-03-19T17:57:23.298896Z","iopub.execute_input":"2022-03-19T17:57:23.299661Z","iopub.status.idle":"2022-03-19T17:57:23.403964Z","shell.execute_reply.started":"2022-03-19T17:57:23.299606Z","shell.execute_reply":"2022-03-19T17:57:23.403196Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"train_x , valtest_x , train_y , valtest_y = train_test_split(all_padded_sequences , np.asarray(all_targets , dtype=np.float32) , test_size= 0.2)\nval_x , test_x , val_y , test_y = train_test_split(valtest_x , valtest_y , test_size=0.5)","metadata":{"execution":{"iopub.status.busy":"2022-03-19T17:57:23.405156Z","iopub.execute_input":"2022-03-19T17:57:23.405417Z","iopub.status.idle":"2022-03-19T17:57:23.414740Z","shell.execute_reply.started":"2022-03-19T17:57:23.405388Z","shell.execute_reply":"2022-03-19T17:57:23.413936Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"Reshape the sets in appropriate shape with the batch size (32)","metadata":{}},{"cell_type":"code","source":"print(train_x.shape , val_x.shape , test_x.shape , train_y.shape , val_y.shape , test_y.shape)\ntrain_x , train_y = train_x[:(train_x.shape[0]//32)*32 , :] , train_y[:(train_y.shape[0]//32)*32]\nval_x , val_y = val_x[:(val_x.shape[0]//32)*32 , :] , val_y[:(val_y.shape[0]//32)*32]\ntest_x , test_y = test_x[:(test_x.shape[0]//32)*32 , :] , test_y[:(test_y.shape[0]//32)*32]\n","metadata":{"execution":{"iopub.status.busy":"2022-03-19T17:57:23.416054Z","iopub.execute_input":"2022-03-19T17:57:23.416482Z","iopub.status.idle":"2022-03-19T17:57:23.427117Z","shell.execute_reply.started":"2022-03-19T17:57:23.416447Z","shell.execute_reply":"2022-03-19T17:57:23.426478Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"Custom layer for using in our model. It averages the hidden states computed for the entire sequence.\n\n","metadata":{}},{"cell_type":"code","source":"class MergeHiddenStates(tf.keras.layers.Layer):\n    def __init__(self):\n        super(MergeHiddenStates , self).__init__()\n        \n    def call(self , inputs):\n        states = inputs\n        return tf.reduce_mean(states , axis = 1)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-03-19T18:04:04.988966Z","iopub.execute_input":"2022-03-19T18:04:04.989463Z","iopub.status.idle":"2022-03-19T18:04:04.994693Z","shell.execute_reply.started":"2022-03-19T18:04:04.989420Z","shell.execute_reply":"2022-03-19T18:04:04.993934Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"Our RNN Model:\n\n* Pass the input sequence through two embeddings, one initialized with the GloVe embeddings and second which is randomly initialized.\n* We only train the second embedding and keep the GloVe embeddings constant.\n* We concatenate the two embeddings and pass to RNN (GRU/LSTM).\n* We get the entire sequence output from the RNN (hidden states/outpts).\n* We pass this entire sequence to out custom layer to average the hidden states.\n* Then simply pass the average vector to a Dense unit for prediction.\n","metadata":{}},{"cell_type":"code","source":"def create_model(input_shape = (MAX_LEN ,) , vocabsize = NUM_WORDS , emb_dim = 100 , rnn_units = 128 , batch_size = 32):\n    inp = tf.keras.layers.Input(shape=input_shape , batch_size=batch_size , dtype=tf.int32)\n    emb_fixed = tf.keras.layers.Embedding(vocabsize ,\n                                         emb_dim,\n                                         mask_zero=False,\n                                         batch_input_shape=(batch_size , input_shape[0]),\n                                         weights = [embedding_matrix] , trainable=False)\n    \n    emb_train = tf.keras.layers.Embedding(vocabsize ,\n                                         emb_dim,\n                                         mask_zero=False,\n                                         batch_input_shape=(batch_size , input_shape[0]),\n                                         weights = [embedding_matrix] , trainable=True)\n    \n    rnn_unit = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(rnn_units , dropout = 0.2 , return_sequences=True))\n    \n    x1 = emb_fixed(inp)\n    x2 = emb_train(inp)\n    x = tf.keras.layers.Concatenate()([x1 , x2])\n    whole_sequence_output = rnn_unit(x)\n    x = MergeHiddenStates()(whole_sequence_output)\n    x = tf.keras.layers.Dropout(0.3)(x)\n    x = tf.keras.layers.Dense(128 , activation='relu')(x)\n    x = tf.keras.layers.Dropout(0.3)(x)\n    preds = tf.keras.layers.Dense(NUM_CLS , activation='softmax')(x)\n    model= tf.keras.Model(inputs = inp , outputs = preds)\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2022-03-19T18:22:50.755724Z","iopub.execute_input":"2022-03-19T18:22:50.756159Z","iopub.status.idle":"2022-03-19T18:22:50.766907Z","shell.execute_reply.started":"2022-03-19T18:22:50.756128Z","shell.execute_reply":"2022-03-19T18:22:50.766020Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"model = create_model()\nprint(model.summary())","metadata":{"execution":{"iopub.status.busy":"2022-03-19T18:23:11.668450Z","iopub.execute_input":"2022-03-19T18:23:11.668744Z","iopub.status.idle":"2022-03-19T18:23:13.120957Z","shell.execute_reply.started":"2022-03-19T18:23:11.668711Z","shell.execute_reply":"2022-03-19T18:23:13.120148Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\nmodel.compile(loss=loss_obj , optimizer = 'adam' , metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-03-19T18:25:05.881367Z","iopub.execute_input":"2022-03-19T18:25:05.881664Z","iopub.status.idle":"2022-03-19T18:25:05.898691Z","shell.execute_reply.started":"2022-03-19T18:25:05.881634Z","shell.execute_reply":"2022-03-19T18:25:05.898008Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"**Let's Train Our Model**","metadata":{}},{"cell_type":"code","source":"history = model.fit(train_x , train_y , batch_size=32 , epochs=10 , validation_data=(val_x , val_y) , shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-19T18:26:25.800675Z","iopub.execute_input":"2022-03-19T18:26:25.800985Z","iopub.status.idle":"2022-03-19T18:28:11.824346Z","shell.execute_reply.started":"2022-03-19T18:26:25.800955Z","shell.execute_reply":"2022-03-19T18:28:11.823704Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"history_1 = model.fit(train_x , train_y , batch_size=32 , epochs=5 , validation_data=(val_x , val_y) , shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-19T18:28:32.203353Z","iopub.execute_input":"2022-03-19T18:28:32.203628Z","iopub.status.idle":"2022-03-19T18:29:23.296084Z","shell.execute_reply.started":"2022-03-19T18:28:32.203599Z","shell.execute_reply":"2022-03-19T18:29:23.295233Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"test_preds = np.argmax(model.predict(test_x , batch_size=32),axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-03-19T18:30:14.585402Z","iopub.execute_input":"2022-03-19T18:30:14.585716Z","iopub.status.idle":"2022-03-19T18:30:15.676933Z","shell.execute_reply.started":"2022-03-19T18:30:14.585683Z","shell.execute_reply":"2022-03-19T18:30:15.676102Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"class_report = classification_report(test_y , test_preds)\nprint(class_report)","metadata":{"execution":{"iopub.status.busy":"2022-03-19T18:30:59.925666Z","iopub.execute_input":"2022-03-19T18:30:59.926138Z","iopub.status.idle":"2022-03-19T18:30:59.937908Z","shell.execute_reply.started":"2022-03-19T18:30:59.926103Z","shell.execute_reply":"2022-03-19T18:30:59.937239Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"**Make predictions for Test set**","metadata":{}},{"cell_type":"markdown","source":"We basically have to repeat the steps we took for the RNN model.\n\nThis time we will use the entire training set to train our model and use it for final predictions.","metadata":{}},{"cell_type":"code","source":"all_sequences_train = tokenizer.texts_to_sequences(all_cleaned_text)\nall_padded_sequences_train = tf.keras.preprocessing.sequence.pad_sequences(all_sequences_train , maxlen=MAX_LEN)\nall_targets = np.asarray(df_clean_tweets_dedep['label'].tolist())","metadata":{"execution":{"iopub.status.busy":"2022-03-19T18:36:44.899698Z","iopub.execute_input":"2022-03-19T18:36:44.900177Z","iopub.status.idle":"2022-03-19T18:36:45.012203Z","shell.execute_reply.started":"2022-03-19T18:36:44.900144Z","shell.execute_reply":"2022-03-19T18:36:45.011100Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"train_x , train_y = all_padded_sequences_train[:(all_padded_sequences_train.shape[0]//32)*32 , :],all_targets[:(all_targets.shape[0]//32)*32]","metadata":{"execution":{"iopub.status.busy":"2022-03-19T18:38:49.867700Z","iopub.execute_input":"2022-03-19T18:38:49.868012Z","iopub.status.idle":"2022-03-19T18:38:49.872964Z","shell.execute_reply.started":"2022-03-19T18:38:49.867982Z","shell.execute_reply":"2022-03-19T18:38:49.872373Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"all_sequences_test = tokenizer.texts_to_sequences(clean_test_tweets)\nall_padded_sequences_test = tf.keras.preprocessing.sequence.pad_sequences(all_sequences_test , maxlen=MAX_LEN)","metadata":{"execution":{"iopub.status.busy":"2022-03-19T18:41:22.594127Z","iopub.execute_input":"2022-03-19T18:41:22.594857Z","iopub.status.idle":"2022-03-19T18:41:22.650761Z","shell.execute_reply.started":"2022-03-19T18:41:22.594786Z","shell.execute_reply":"2022-03-19T18:41:22.649912Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"all_padded_sequences_test_f = np.concatenate([all_padded_sequences_test , np.asarray(all_padded_sequences_test[-1,:]).reshape((1, MAX_LEN))],axis = 0)\nprint(all_padded_sequences_test_f.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-19T18:44:43.015408Z","iopub.execute_input":"2022-03-19T18:44:43.016362Z","iopub.status.idle":"2022-03-19T18:44:43.023029Z","shell.execute_reply.started":"2022-03-19T18:44:43.016289Z","shell.execute_reply":"2022-03-19T18:44:43.021860Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"Create model for prediction and compile\n\n","metadata":{}},{"cell_type":"code","source":"prediction_model = create_model()\ntest_loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\nprediction_model.compile(loss=test_loss_obj, optimizer='adam', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-03-19T18:48:25.772015Z","iopub.execute_input":"2022-03-19T18:48:25.772624Z","iopub.status.idle":"2022-03-19T18:48:26.307182Z","shell.execute_reply.started":"2022-03-19T18:48:25.772586Z","shell.execute_reply":"2022-03-19T18:48:26.306232Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"history = prediction_model.fit(train_x , train_y , batch_size=32 , epochs=10 , shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-03-19T18:48:26.901624Z","iopub.execute_input":"2022-03-19T18:48:26.901905Z","iopub.status.idle":"2022-03-19T18:50:39.011536Z","shell.execute_reply.started":"2022-03-19T18:48:26.901876Z","shell.execute_reply":"2022-03-19T18:50:39.010706Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":"Make predictions\n\n","metadata":{}},{"cell_type":"code","source":"test_predictions = prediction_model.predict(all_padded_sequences_test_f)\ntest_predictions_classes = np.argmax(test_predictions, axis=-1)[:-1]","metadata":{"execution":{"iopub.status.busy":"2022-03-19T18:52:43.544047Z","iopub.execute_input":"2022-03-19T18:52:43.544729Z","iopub.status.idle":"2022-03-19T18:52:45.504031Z","shell.execute_reply.started":"2022-03-19T18:52:43.544680Z","shell.execute_reply":"2022-03-19T18:52:45.503233Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":"Save to file for submission\n\n","metadata":{}},{"cell_type":"code","source":"test_ids = test_data['id']\npred_df = pd.DataFrame(data={\"id\":test_ids, \"target\":test_predictions_classes})","metadata":{"execution":{"iopub.status.busy":"2022-03-19T18:53:49.403328Z","iopub.execute_input":"2022-03-19T18:53:49.404168Z","iopub.status.idle":"2022-03-19T18:53:49.413876Z","shell.execute_reply.started":"2022-03-19T18:53:49.404126Z","shell.execute_reply":"2022-03-19T18:53:49.412658Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"pred_df.to_csv(\"submission.csv\", index=False)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-19T18:54:18.233518Z","iopub.execute_input":"2022-03-19T18:54:18.233834Z","iopub.status.idle":"2022-03-19T18:54:18.245889Z","shell.execute_reply.started":"2022-03-19T18:54:18.233801Z","shell.execute_reply":"2022-03-19T18:54:18.244918Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}